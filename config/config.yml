board_size: 10
pieces_per_player: 5
die_sides: 4

# Training parameters
alpha: 0.001               # Learning rate for TD(Lambda)
discount_factor: 0.95      # How much to value future rewards (gamma)
batch_size: 128            # Number of experiences to use for each training update
epsilon_start: 1.0         # Start with 100% random moves
epsilon_end: 0.01          # End with 1% random moves
epsilon_decay: 0.9995      # Slower decay to allow for more exploration
replay_buffer_size: 50000  # Much larger buffer for diverse and stable learning
evaluation_frequency: 100  # Evaluate the model every 100 episodes
evaluation_games: 100      # Number of games to play during evaluation
num_epochs: 10             # Number of training epochs
games_per_epoch: 100       # Number of self-play games to generate per epoch

# TD(Lambda) parameters
lambda_start: 0.9          # Start with high reliance on Monte Carlo (final outcome)
lambda_end: 0.5            # End with more reliance on TD (AI's own predictions)
lambda_decay: 0.999        # Decay rate for lambda per episode
